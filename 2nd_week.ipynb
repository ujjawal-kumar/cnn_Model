{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujjawal/project/env1/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "MNIST = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .01 # the learning rate\n",
    "batch_size = 128 # the number of examples we will consider per iterations\n",
    "n_epochs = 2500 # the number of iterations we will do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create placeholders for X (our features) and Y (our labels)\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create Variables for w (our weights) and b (our biases)\n",
    "w = tf.Variable(tf.truncated_normal(shape = [784, 10], stddev=0.01), name = 'w')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_logits = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-cb405794c09c>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: write code to compute the cross_entropy_loss and the mean_squared_loss.\n",
    "# Experiment with both losses: which one performs better? Why might this be?\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits))\n",
    "mean_squared_loss = tf.reduce_mean(tf.square(Y - normalized_logits))\n",
    "loss = mean_squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a GradientDescentOptimizer that minimizes our loss. \n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operations that help us monitour our accuracy\n",
    "cp = tf.equal(tf.argmax(logits, axis = 1), tf.argmax(Y, axis = 1))\n",
    "acc= tf.reduce_mean(tf.cast(cp, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08995632082223892\n",
      "Loss: 0.08774362504482269\n",
      "Loss: 0.08516256511211395\n",
      "Loss: 0.08176962286233902\n",
      "Loss: 0.07943440228700638\n",
      "test acc: 0.6502000093460083\n"
     ]
    }
   ],
   "source": [
    "# TODO: create a global_variables_initializer, launch the graph, and run the optimization step for n_epochs iterations.\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "for i in range(n_epochs):\n",
    "    batch = MNIST.train.next_batch(batch_size)\n",
    "    sess.run(opt, feed_dict = {X: batch[0], Y: batch[1]})\n",
    "    if i % 500 == 0:\n",
    "        l = loss.eval(feed_dict = {X: batch[0], Y: batch[1]})\n",
    "        print(\"Loss: {}\".format(l))\n",
    "a = acc.eval(feed_dict = {X: MNIST.test.images, Y: MNIST.test.labels})\n",
    "print(\"test acc: {}\".format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images(images):\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(1, 5, i + 1) # Plotting 1 row of NUM_FIGURES.\n",
    "        plt.axis('off')\n",
    "        plt.imshow(images[i].reshape((28,28)), cmap = plt.cm.gray_r)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffa80862fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted 3, ground truth was 8\n",
      "predicted 7, ground truth was 9\n",
      "predicted 6, ground truth was 6\n",
      "predicted 3, ground truth was 3\n",
      "predicted 1, ground truth was 1\n"
     ]
    }
   ],
   "source": [
    "# TODO: Take some random images from the MNIST dataset, run a prediction, and display the prediction and the actual label.\n",
    "\n",
    "NUM_IMAGES = 5\n",
    "rand_image_idx = np.random.randint(0, MNIST.test.images.shape[1], NUM_IMAGES)\n",
    "images, labels = [MNIST.test.images[i] for i in rand_image_idx], [MNIST.test.labels[i] for i in rand_image_idx]\n",
    "images, labels = np.array(images), np.array(labels)\n",
    "plot_images(images)\n",
    "for i in range(images.shape[0]):\n",
    "    prediction = sess.run(logits, feed_dict = {X: images[i].reshape((1, images[i].shape[0])), Y: labels[i].reshape((1, labels[i].shape[0]))})\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    actual_label = np.argmax(labels[i])\n",
    "    print(\"predicted {}, ground truth was {}\".format(predicted_label, actual_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
